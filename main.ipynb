{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c726972e",
   "metadata": {},
   "source": [
    "# Data Processing Examples\n",
    "Example code for downloading and preparing code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4b2f80",
   "metadata": {},
   "source": [
    "## 0 - Misc Imports\n",
    "Plenty of imports that are generally useful -- imports up here so they don't cause too much clutter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f01d46c",
   "metadata": {},
   "source": [
    "### Important Imports\n",
    "Actually important imports -- make sure these dependencies are installed (will do a requirements.txt at some point); this code block doesn't actually *have* to be run; any required imports will be at the top of each code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f420b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0569d4b",
   "metadata": {},
   "source": [
    "#### Environment Settings\n",
    "To be honest, I have no clue where this is supposed to go (like idk what is causing errors), but this **does** need to be run at some point before the rest of the code ðŸ˜Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37ce747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SCIPY_ARRAY_API'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fe5a01",
   "metadata": {},
   "source": [
    "## 1 - Download + Pre-Process\n",
    "Essentially example code for `forest_fire/prepare_data`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4852cb",
   "metadata": {},
   "source": [
    "First, we need to download the data -- in my experience this tends to take circa 10-12 ish minutes per month of data, so for 5 years, that adds up to around 6 hours (I think). In other words, probably set this cell off and leave it going in the background.\n",
    "\n",
    "DO NOTE THAT this currently only downloads the training data (2010-2014); downloading test data (2015-2019) requires changes to the code.\n",
    "\n",
    "Either:\n",
    "1. Change `request_total_data` to request all 10 years of data at once, or\n",
    "2. Make a second request for the test data\n",
    "\n",
    "I have gone with 2. while doing the project, but 1. might be more efficient, with the small caveat that it is a bit of a pain if the download is interrupted partway. Note that in the second case, the training data can be used as the prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a7d920",
   "metadata": {},
   "outputs": [],
   "source": [
    "from main.prepare_data.CDS_requests import request_total_data\n",
    "\n",
    "## USE CANADA RANGE AS EXAMPLE\n",
    "from main.prepare_data.extents import CANADA_RICHARDSON_EXTENT\n",
    "\n",
    "request_total_data(\n",
    "    extent = CANADA_RICHARDSON_EXTENT,\n",
    "    data_path = './data'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f3a39b",
   "metadata": {},
   "source": [
    "By default, the above code block will download our data to `data/canada/main/combined.grib` and `data/canada/prior/combined.grib`\n",
    "\n",
    "For future reference, it'll be much quicker to have this data in the form of Zarr groups instead of grib files, so we do that next -- in fairness, this step isn't entirely necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb38d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from main.prepare_data.process_data import grib_to_zarr\n",
    "\n",
    "grib_to_zarr(grib_path = 'data/canada/main/combined.grib', zarr_path = 'data/canada/main/', name = '_ZARR')\n",
    "grib_to_zarr(grib_path = 'data/canada/prior/combined.grib', zarr_path = 'data/canada/prior', name = '_ZARR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ae10e8",
   "metadata": {},
   "source": [
    "We now need to **setup** the data -- for this we have the `setup_dataset()` function from `process_data.py`. We will write this prepared dataset to storage as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b81747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from main.prepare_data.process_data import setup_dataset\n",
    "\n",
    "PROXIES = {\n",
    "    'tp': (30, 90, 180),\n",
    "    't2m': (30, 90, 180)\n",
    "}\n",
    "\n",
    "setup_dataset(\n",
    "    main_path = 'data/canada/main/_ZARR',\n",
    "    prior_path = 'data/canada/prior/_ZARR',\n",
    "    proxy_config = PROXIES\n",
    ").to_zarr(\n",
    "    store = 'data/canada/ZARR_READY'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f7cd08",
   "metadata": {},
   "source": [
    "Any further manipulation of data is then handled in `forest_fire/train`. You can also restart the kernel at this point to clear stored variables if you so wish -- just remember to re-import modules from section 0!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e746a5c",
   "metadata": {},
   "source": [
    "## 2 - Model Training\n",
    "Example code for `forest_fire/train` + examples of models used for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f817d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from main.train.prepare_samples import prep_samples\n",
    "\n",
    "train_ds = xr.open_zarr(\n",
    "    'data/_ZARR_READY/canada'\n",
    ")\n",
    "\n",
    "X, y = prep_samples(\n",
    "    ds = train_ds,\n",
    "    compute_chunks = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed30f7c4",
   "metadata": {},
   "source": [
    "### Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97eb27e7",
   "metadata": {},
   "source": [
    "This generally works fine. However, the data for this project has tended to be wildly imbalanced (with around a 1:10000 ratio of negative-positive points being fairly common). Therefore, up/downsampling has been used on the training data. This has been done using the `imblearn` module.\n",
    "\n",
    "Downsampling was done with `RandomUnderSampler` (bootstrapped samples), while upsampling was done with a mix of `ADASYN` and `SMOTE`. Example code for this is provided below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42a6878",
   "metadata": {},
   "source": [
    "#### Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01a1027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "downsampler = RandomUnderSampler(sampling_strategy = 0.1)\n",
    "\n",
    "X_down, y_down = downsampler.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f70de0b",
   "metadata": {},
   "source": [
    "#### Upsampling\n",
    "Examples are provided for both SMOTE and ADASYN; there is more work to be done on comparing which is better in various cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5dc452",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''SMOTE'''\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "SMOTE_oversampler = SMOTE(sampling_strategy = 0.1)\n",
    "\n",
    "X_smote, y_smote = SMOTE_oversampler.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8a0df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ADASYN'''\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "ADASYN_oversampler = ADASYN(sampling_strategy = 0.1)\n",
    "\n",
    "X_ada, y_ada = ADASYN_oversampler.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9864fc",
   "metadata": {},
   "source": [
    "### Training\n",
    "Various tree-based models were used -- *Random Forests* were tried at first, but later it turned out that *AdaBoost* worked out much better. Further changes to other Boosting models may result in better performance (eg *XGBoost*). All of the models used were imported from `sklearn` - further optimisations may be possible via custom-written trees (although these tend to be written sufficiently well)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5152ef0d",
   "metadata": {},
   "source": [
    "#### Optional Import(s)\n",
    "Intel has put out the `sklearnex` module which provides speed ups for training of sklearn models on Intel CPUs. If this is not relevant, ignore this section. If this section raises errors, also just skip it; it provides a speed up but the functionality is completely unaffected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1803048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e977e7",
   "metadata": {},
   "source": [
    "#### Training models\n",
    "Here, we use `X_down` and `y_down` for training data, but can very much change to any other set of resampled (or potentially not) data.\n",
    "\n",
    "I would recommend using resampled data of some sort since imbalanced data has a tendency to fuck over the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aaf6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_jobs=-1, n_estimators=100)\n",
    "model.fit(X_down, y_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6461cbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## AdaBoost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "model = AdaBoostClassifier()\n",
    "model.fit(X_down, y_down)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad513b6c",
   "metadata": {},
   "source": [
    "#### Model I/O\n",
    "We use joblib to save the models into some sort of binary form. I have no clue what functionality is saved here, but you can definitely use the models to get predictions which is the main thing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cf5d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save Model\n",
    "from joblib import dump\n",
    "\n",
    "dump(model, 'model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4469d72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Model\n",
    "from joblib import load\n",
    "\n",
    "model = load('model.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_ff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
