{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eebc7eb",
   "metadata": {},
   "source": [
    "# The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e5fe34",
   "metadata": {},
   "source": [
    "## reloading\n",
    "sometimes it helps to reload data -- redo imports up here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e6292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f287590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neural_net as nen\n",
    "reload(nen)\n",
    "from neural_net import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bec8a3",
   "metadata": {},
   "source": [
    "## setup\n",
    "Import, load data and instantiate relevant classes (DataLoader, Dataset etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364926d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports\n",
    "#  modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# custom code\n",
    "from dask_addons import FlattenedDaskDataset\n",
    "from neural_net import DNN, geoDataset, BatchDataLoader, train, test\n",
    "from open_fire_data import FlattenedTruthTable\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d68d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 21:22:14,760 - distributed.scheduler - WARNING - Detected different `run_spec` for key ('rechunk-split-0fc45d5b86da226cc122bcb4970315c5', 183) between two consecutive calls to `update_graph`. This can cause failures and deadlocks down the line. Please ensure unique key names. If you are using a standard dask collections, consider releasing all the data before resubmitting another computation. More details and help can be found at https://github.com/dask/dask/issues/9888. \n",
      "Debugging information\n",
      "---------------------\n",
      "old task state: released\n",
      "old run_spec: Alias(('rechunk-split-0fc45d5b86da226cc122bcb4970315c5', 183)->('getitem-rechunk-split-0fc45d5b86da226cc122bcb4970315c5', 183))\n",
      "new run_spec: <Task ('rechunk-split-0fc45d5b86da226cc122bcb4970315c5', 183) getitem(...)>\n",
      "old dependencies: {('getitem-rechunk-split-0fc45d5b86da226cc122bcb4970315c5', 183)}\n",
      "new dependencies: frozenset({('getitem-83af0cb3d308ebcd323c15a7659ef338', 1, 0, 1)})\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f57006",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR_PATH = '../data/_ZARR_FILES'\n",
    "\n",
    "data_path = os.path.join(DATA_DIR_PATH, 'alaska_full.zarr')\n",
    "prior_data_path = os.path.join(DATA_DIR_PATH, 'alaska_prior.zarr')\n",
    "\n",
    "data = xr.open_zarr(data_path, decode_timedelta=False)\n",
    "prior_data = xr.open_zarr(prior_data_path, decode_timedelta=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf9ca84",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ds = FlattenedDaskDataset(data, prior_data)\n",
    "input_ds.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c191e8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_data = FlattenedTruthTable(\n",
    "    pd.read_csv('../data/_FIRE/alaska_range_csv/data.csv'),\n",
    "    lat_vals = input_ds.latitude,\n",
    "    long_vals = input_ds.longitude\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaeff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = geoDataset(input_ds, fire_data, feature_num = 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d4ca7e",
   "metadata": {},
   "source": [
    "## training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072e8e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0285d28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b564146",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = BatchDataLoader(ds, end=100)\n",
    "test_dataloader = BatchDataLoader(ds)\n",
    "\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3675fdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41386b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer, device)\n",
    "    test(test_dataloader, model, loss_fn, device)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_ff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
